{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-2\n",
    "\n",
    "---\n",
    "\n",
    "**text preprocessing, document similarity, and a project**\n",
    "\n",
    "Our first foray into Natural Language Processing didn't go very deep, so now it's time to dig in a little. We're returning to the jobs data - I'll be using the pickle file in the data folder, so if something I'm writing doesn't make sense, it's probably because we're using different data.\n",
    "\n",
    "For NLP projects, you should expect to spend around 80% of your time on data prep. Why so much? Text data can be extremely messy, and if you haven't worked with a particular dataset before then you'll probably have to circle back to the preprocessing stage multiple times. It's definitely worth it though, because extracting insights from text data can have significant value for a business; whether you're analyzing the sentiment of tweets linked to followers of your business, using topic modeling on call transcripts, or detecting fradulent transactions.\n",
    "\n",
    "If you find that this notebook piques your interest, then I'd recommend that you explore the docs of [sklearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), [spacy](https://spacy.io/), [nltk](https://www.nltk.org/), and [gensim](https://radimrehurek.com/gensim/). There's are a ton of topics to cover (from dimensionality reduction to cosine similarity), each one of them will take some time to sink in, and learning how and when to use them all is truly an art. I can only go so far into the bits and pieces that are covered below.\n",
    "\n",
    "At the end of this one there's a project. It's on you to do it or not - obviously there aren't any grades to go around - but if you do choose to tackle it, I'd be happy to give guidance or feedback.\n",
    "\n",
    "**Contents:**\n",
    "1. text preprocessing: \n",
    "  * tokenization, removal, stemming\n",
    "  * tfidf\n",
    "2. text analysis:\n",
    "  * document similarity\n",
    "  * word importances\n",
    "  * word clouds\n",
    "3. project idea\n",
    "\n",
    "---\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "Always start off by reading through some of your documents. If you don't know the context of your text, then your analysis will suffer, your models won't make sense, blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data storage\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw data\n",
    "# you can move the file to this folder, or rename 'jobs.pkl' to its full path\n",
    "# something like \"~/Documents/Cult-Terry/data/jobs.pkl\"\n",
    "\n",
    "raw_jobs_data = pickle.load(open('jobs.pkl', 'rb'))\n",
    "\n",
    "example = raw_jobs_data[0][1]  # [first job][job description]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word, symbol, number is its own entity (token)\n",
    "# this step can be modified to exclude words of certain length,\n",
    "# non alphanumeric symbols, etc\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(example)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech tagging is a difficult task. It's case sensitive, so\n",
    "# try opening up more words and you'll see that most capitalized words\n",
    "# are labeled as propper nouns (NNP)\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "tokens = pos_tag(word_tokenize(example))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# removing propper nouns and punctuation\n",
    "clean_tokens = [t for t in tokens if t[1] != 'NNP' and t[0] not in string.punctuation]\n",
    "clean_text = ' '.join([t[0] for t in clean_tokens])  # stitching the text back together\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bunch of non-alphanumeric characters made it through\n",
    "import re\n",
    "\n",
    "# this regex statement will get rid of all non-letters, and make our text lower-case\n",
    "cleaner_text = re.sub('[^a-zA-Z]', ' ', clean_text).lower()\n",
    "cleaner_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already saw this in NLP-1, CountVectorizer gives counts of all the words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()  # initalize the class\n",
    "X = cv.fit_transform([cleaner_text])  # cv reads iterables, so pass the string as a list object\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is the process of using an algorithm for suffix stripping\n",
    "# ie. cats -> cat  running -> run  industrialized -> industrial\n",
    "# there are various algorithms out there, but nltk's PorterStemmer\n",
    "# is derived from Martin Porter's algorithm that he proposed in 1980\n",
    "# (and then continued to update for decades)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in cleaner_text.split(' ')]\n",
    "stemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is a brute-force approach to dimensionality reduction, or\n",
    "# normalization, of text data. We can also drop stop-words for further\n",
    "# d-reduction\n",
    "\n",
    "stemmed_text = ' '.join(stemmed)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')  # stop_words='english' is the default\n",
    "X = cv.fit_transform([stemmed_text])\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's break to a toy dataset to gently segue into tfidf\n",
    "\n",
    "# say we have the following 5 sentences\n",
    "\n",
    "sent_1 = 'cold weather events are bad for roads'\n",
    "sent_2 = 'she thinks that cold sandwhiches are best'\n",
    "sent_3 = 'he thinks hot sandwhiches with mustard taste better'\n",
    "sent_4 = 'it was a cold and dreary morning'\n",
    "sent_5 = 'we need freezers that keep ice cold'\n",
    "all_sentences = [sent_1, sent_2, sent_3, sent_4, sent_5]\n",
    "\n",
    "# with simple counts our data looks like this\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(all_sentences)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare our documents we can use cosine similarity.\n",
    "# cosine similarity measures the angle between two vectors, if they point\n",
    "# in exactly the same direction (have all the same words) we get a 1\n",
    "# ...hit command+tab on cosine_similarity(X) below to read more\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the closest match to 'she thinks that cold sandwhiches are best' is\n",
    "# cold weather events are bad for roads' and 'we need freezers that keep ice cold'\n",
    "# even though contextually we know that 'she preffers hot sandwhiches that have mustard'\n",
    "# is closer.\n",
    "# we need to weight the words so that 'sandwhiches' is more important than 'cold'\n",
    "\n",
    "# tf-idf: term frequency inverse document frequency, is just the tool we need\n",
    "# cv gave us the term frequency, so the new part is the -idf\n",
    "# you can find the math here: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# but essentially we're going to weight words by their appearance across all sentences\n",
    "# and use that as a divisor to the term frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# corrected data\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(all_sentences)\n",
    "pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names()).round(2)  # compare scores of 'cold' and 'sandwhiches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we can see that the sandwhich sentences are closest\n",
    "\n",
    "cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to our original data, using tfidf\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = tfidf.fit_transform([stemmed_text])\n",
    "pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to gather more context we can include bigrams, or groups of two words\n",
    "# ex: 'the dog ran' -> ['the dog', 'dog ran']\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english',\n",
    "                        ngram_range=(1, 2))\n",
    "\n",
    "X = tfidf.fit_transform([stemmed_text])\n",
    "pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# this more than doubles the number of features that we have, but if we\n",
    "# exclude words and bigrams that appear in almost every document or in\n",
    "# very few, then we can reduce the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean all of our documents, we can copy paste all of our\n",
    "# previous code and throw it into a function. We can even save this\n",
    "# off in a .py script for other projects, modifying parts if we need\n",
    "\n",
    "def text_cleaner(raw_docs):\n",
    "    '''\n",
    "    general text cleaning for a list of documents\n",
    "    '''\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.tag import pos_tag\n",
    "    import string\n",
    "    import re\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    clean_docs = []\n",
    "    \n",
    "    for doc in raw_docs:\n",
    "        \n",
    "        tokens = pos_tag(word_tokenize(doc))\n",
    "\n",
    "        clean_tokens = [t for t in tokens if t[1] != 'NNP' and t[0] not in string.punctuation]\n",
    "        clean_text = ' '.join([t[0] for t in clean_tokens])\n",
    "\n",
    "        cleaner_text = re.sub('[^a-zA-Z]', ' ', clean_text).lower()\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "        clean_docs.append(' '.join([stemmer.stem(word) for word in cleaner_text.split(' ')]))\n",
    "\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all of the documents\n",
    "clean_docs = text_cleaner([doc[1] for key, doc in raw_jobs_data.items()])\n",
    "\n",
    "# vectorize\n",
    "tfidf = TfidfVectorizer(stop_words='english',\n",
    "                        max_df=0.8,  # remove features that appear in > 80% of documents\n",
    "                        min_df=0.1,  # and features that appear in < 10%\n",
    "                        ngram_range=(1, 3))  # uni, bi, and trigrams\n",
    "\n",
    "X = tfidf.fit_transform(clean_docs)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can grab cosine similarity to find which jobs are similar\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(tfidf_df))\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we go through each row and find the top two numbers we'll get\n",
    "# the document we're looking at (because it's closest to itself) and\n",
    "# the most similar document to it\n",
    "\n",
    "top_pairs = []\n",
    "\n",
    "for i in similarities.index:  # iterate over the indices\n",
    "    top = similarities.nlargest(2, i)[i].drop(i)  # reffer to self, and drop self to keep closest match\n",
    "    top_pairs.append((i, top.index[0], top.values[0]))  # store self, index of match, and cosine\n",
    "    \n",
    "sorted(top_pairs, key=lambda x: x[2], reverse=True)[:5]  # sort by cosine and show the top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm... something is fishy\n",
    "\n",
    "len(similarities.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some jobs were posted multiple times, so drop the duplicates\n",
    "similarities = similarities.drop_duplicates()\n",
    "\n",
    "\n",
    "# and try again\n",
    "top_pairs = []\n",
    "\n",
    "for i in similarities.index:\n",
    "    top = similarities.nlargest(2, i)[i].drop(i)\n",
    "    top_pairs.append((i, top.index[0], top.values[0]))\n",
    "    \n",
    "sorted(top_pairs, key=lambda x: x[2], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top two seem remarkably close, check that they aren't duplicates\n",
    "\n",
    "raw_jobs_data[67][0], raw_jobs_data[188][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for a visualization of our words - word clouds!\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# copy pasted from notebook 05\n",
    "\n",
    "def make_cloud(freq_dict):\n",
    "    '''make a wordcloud from a word frequency dictionary'''\n",
    "    \n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word importances can be pulled from tfidf directly by taking the idf score\n",
    "# and matching that to the word or bigram\n",
    "\n",
    "word_importances = list(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "\n",
    "word_importances = sorted(word_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_importances[:10]  # the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these words are helpful, and some are just from the EOO statements \n",
    "\n",
    "make_cloud(dict(word_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare with counts (and similar parameters)\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 3),\n",
    "                     max_df=0.8,\n",
    "                     min_df=0.1,\n",
    "                     stop_words='english')\n",
    "vocab = cv.fit_transform(clean_docs)\n",
    "wordcount_df = pd.DataFrame(data=vocab.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "sorted_words = wordcount_df.sum().sort_values(ascending=False)\n",
    "\n",
    "make_cloud(sorted_words.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Project Idea\n",
    "\n",
    "The best way to learn coding, or anything, is practice. So, here's a project idea for those of you who want to put it on your resume, or want to improve your skills, or are just bored. It will require knowledge from the previous notebooks.\n",
    "\n",
    "**Project:**\n",
    "\n",
    "What insight can you derive from job descriptions using NLP, and supervised or unsupervised learning methods? The result can be clusters of job descriptions, a classification model, etc.\n",
    "\n",
    "**Data:**\n",
    "\n",
    "* whatever you already have\n",
    "* more jobs scraped from the same or a different source\n",
    "\n",
    "**Tools:**\n",
    "\n",
    "* selenium\n",
    "* pandas/numpy\n",
    "* matplotlib/seaborn/wordcloud\n",
    "* sklearn/nltk\n",
    "* any others you need\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "* data scraping\n",
    "* data visualization\n",
    "* text data preprocessing\n",
    "* supervised/unsupervised modeling of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
