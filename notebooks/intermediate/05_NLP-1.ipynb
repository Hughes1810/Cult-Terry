{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-1\n",
    "\n",
    "---\n",
    "\n",
    "**word clouds, intro to NLP and sklearn**\n",
    "\n",
    "Way way way back, pre MSBA bootcamp we were supposed to use some word-cloud generating software. Well now we're going to do that with python, and at the end we'll have the ability to create our own word clouds, and even manipulate text data.\n",
    "\n",
    "**Contents:**\n",
    "1. Words as Data\n",
    "2. Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Words as Data:\n",
    "\n",
    "In the data scraping notebook, we grabbed a bunch of job descriptions from Monster and saved them in a pickle file - if you skipped this one, use the file \"jobs.pkl\" in the data folder. \n",
    "\n",
    "Let's check out some of the text, and see how we can potentially turn it into something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data storage\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw data\n",
    "# if you need the file in data, change the 'jobs.pkl' path\n",
    "# it'll look something like \"~/Documents/uga/Cult-Terry/data/jobs.pkl\"\n",
    "\n",
    "raw_jobs_data = pickle.load(open('jobs.pkl', 'rb'))\n",
    "\n",
    "raw_jobs_data[0][0] # [first job][title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '\\n' is the symbol for a line break\n",
    "# use the builtin function to split the string on this character\n",
    "\n",
    "first_job = raw_jobs_data[0]\n",
    "description = first_job[0]\n",
    "\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should see the first chunk as \"job title at company\"\n",
    "# ideally everything is formatted this way\n",
    "\n",
    "# using a list comprehension to view all job titles\n",
    "\n",
    "all_titles = [raw_jobs_data[job][0].split('\\n')[0]\n",
    "              for job in raw_jobs_data.keys()]\n",
    "\n",
    "all_titles[:5], all_titles[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like the data I got isn't entirely \"data scientist\" roles\n",
    "# since that's the keyword I'm looking for, I'll optimize my results\n",
    "\n",
    "data_scientist_titles = [job for job in all_titles\n",
    "                         if 'data scientist' in job]\n",
    "len(data_scientist_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huh, no jobs\n",
    "\n",
    "# let's check some things\n",
    "\n",
    "print('data scientist' in 'Senior Data Scientist at')\n",
    "print('Data Scientist' in 'Senior Data Scientist at')\n",
    "print('Data Scientist' in 'DATA SCIENTIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shouldn't be surprising, text is case sensitive after all\n",
    "# all we have to do is standardize before we operate\n",
    "\n",
    "print('data scientist' in 'Senior Data Scientist at'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making everything lower case\n",
    "# take the index of each job, so we can reference that to the job descriptions\n",
    "\n",
    "data_scientist_titles = [(job.lower(), idx) for idx, job in enumerate(all_titles)\n",
    "                         if 'data scientist' in job.lower()]\n",
    "len(data_scientist_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've cut down our data substantially, but seeing as everything removed\n",
    "# was unrelated to our outcome, that means the data is substantially cleaner\n",
    "\n",
    "# moving on to the job descriptions\n",
    "# we can use the indices attached to each title to grab descriptions\n",
    "\n",
    "last_index = data_scientist_titles[-1][1]\n",
    "\n",
    "last_description = raw_jobs_data[last_index][1]\n",
    "\n",
    "last_description.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work towards this goal: find the most frequent words for data scientist jobs\n",
    "\n",
    "# our first step is to \"clean\" last_description by turning it into a list of words\n",
    "\n",
    "# clean = remove punctuation, numbers, and white space, lower-case everything \n",
    "\n",
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "print(string.digits)\n",
    "\n",
    "clean_description = last_description.lower()\n",
    "for i in string.punctuation + string.digits:  # full list of digits and punctuation\n",
    "    clean_description = clean_description.replace(i, ' ')\n",
    "    \n",
    "clean_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need to remove '\\n' and whatever ’ is, just copy pase into a replace\n",
    "\n",
    "clean_description = clean_description.replace('\\n', ' ').replace('’', ' ')\n",
    "\n",
    "# now split on whitespace\n",
    "\n",
    "clean_description = clean_description.split(' ')\n",
    "\n",
    "clean_description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every word that isn't blank\n",
    "\n",
    "clean_description = [word for word in clean_description if len(word) > 0]\n",
    "\n",
    "clean_description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter, pretty self explanatory\n",
    "# most_common gives the most common items\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(clean_description).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**a few points to make here**\n",
    "\n",
    "1. As you can see, words like \"and,\" \"of,\" \"to\" are very common but don't give us any information at all. Noise words like this are called \"stop words.\"\n",
    "2. Additionally, looking at just single words doesn't look like it's giving us much to go off of. Bi-grams and tri-grams (series of two and three words) would surely give use more information.\n",
    "3. Some non-stop words like \"data\" are probably not going to be that helpful.\n",
    "\n",
    "**sklearn**\n",
    "\n",
    "sci-kit learn can help us handle all of these concerns. This marks a pretty big point for us analytically, sklearn permeates almost every angle of data analysis/science where python is concerned (unless you want to build everything from scratch).\n",
    "\n",
    "If you ever struggle with it, **read the docs**. I can't emphasize how well most of the documentation is done for sklearn. Most of the examples are solid, most come with visualizations, and almost all of them are accessible with very little background knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - the frontline tool for turning text into data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pre-cleaned description \n",
    "\n",
    "test_text = [last_description.lower()]  # CV iterates over lists of documents, start with one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting off with single words\n",
    "\n",
    "cv = CountVectorizer()  # CV is a class, so we have to initialize it, give it a place to be called from\n",
    "\n",
    "vocab = cv.fit_transform(test_text)  # fit on the text, transform into calculable fields\n",
    "\n",
    "single_grams = cv.get_feature_names()  # words are called \"feature names\"\n",
    "\n",
    "print(len(single_grams))\n",
    "single_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on their own, most of these words just don't say anything to us. Certainly some words\n",
    "# like python and hadoop are fine, but without context most words are meaningless\n",
    "\n",
    "# moving on to bigrams - two words\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "vocab = cv.fit_transform(test_text)\n",
    "\n",
    "double_grams = cv.get_feature_names()\n",
    "\n",
    "print(len(double_grams))\n",
    "double_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so, some of these word pairs are starting to make sense BUT we still need frequencies\n",
    "\n",
    "vocab.toarray()  # toarray returns the counts for each - in this case bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we have a single row made of all the counts of bigrams in this one job description. What happens when we run CountVectorizer with this description and another?\n",
    "\n",
    "Some of the bigrams will be overlapping (adding values into the same column), some will be new (adding new columns). The end result will be a sparse matrix with as many rows as we have documents, and as many columns as there are unique bigrams.\n",
    "\n",
    "Sparse matricies are \"mostly empty,\" in this case most jobs aren't with any one company,\n",
    "aren't requiring exactly the same skills, and aren't worded the same way. The end result\n",
    "are lots of 0's in each row, and without compression it would take way longer for your\n",
    "machine to load. Compression means that each non-zero cell has a reference to it's row,\n",
    "column, and value which can be used to build a dataframe where non-referenced cells\n",
    "are known to be 0.\n",
    "\n",
    "Since we want to build clouds of the most frequent unigrams and bigrams, all we have to do is throw CountVectorizer at our entire document-space, and then sum up our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all descriptions from our raw data using the data scientist keys\n",
    "\n",
    "all_text = [raw_jobs_data[key[1]][1] for key in data_scientist_titles]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 2), stop_words='english')  # both uni and bigrams, and remove stopwords\n",
    "\n",
    "vocab = cv.fit_transform(all_text)  # fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of jobs x number of uni and bigrams\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not easy to tell what's what\n",
    "\n",
    "count_matrix = vocab.toarray()\n",
    "\n",
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can put this all into a dataframe though, and make it a little easier to comprehend\n",
    "\n",
    "wordcount_df = pd.DataFrame(data=count_matrix, columns=cv.get_feature_names())\n",
    "\n",
    "wordcount_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've certainly captured some interesting looking word combinations\n",
    "# let's sum-up our columns and look at the most frequent ones\n",
    "\n",
    "sorted_words = wordcount_df.sum().sort_values(ascending=False)\n",
    "\n",
    "sorted_words[:10]  # all of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick way to reference our bigrams\n",
    "\n",
    "bigram_indices = [word for word in sorted_words.index if ' ' in word]\n",
    "\n",
    "sorted_words[bigram_indices][:10]  # only bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Word Clouds\n",
    "\n",
    "Time to use the eponymous wordcloud package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to install a package inline\n",
    "\n",
    "# if you run into \"ModuleNotFoundError: No module named 'whatever'\"\n",
    "# this is your first line of defence\n",
    "\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is copy paste from a google search of making a wordcloud\n",
    "\n",
    "def make_cloud(freq_dict):\n",
    "    '''make a wordcloud from a word frequency dictionary'''\n",
    "    \n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of frequencies from our sorted vocab, and call the function\n",
    "\n",
    "freq_dict = sorted_words.to_dict()\n",
    "\n",
    "make_cloud(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool, we can do the same thing with just our bigrams\n",
    "\n",
    "make_cloud(sorted_words[bigram_indices].to_dict())  # all in one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a lot of our frequent bigrams come from EEO statements. If the formats are all the same we can hack those out pretty easily.\n",
    "\n",
    "However, for little effort and no manual cleaning this looks pretty good. \n",
    "\n",
    "To recap, we can get to this point with 6 lines of code:\n",
    "\n",
    "```\n",
    "cv = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "vocab = cv.fit_transform(all_text)\n",
    "wordcount_df = pd.DataFrame(data=vocab.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "sorted_words = wordcount_df.sum().sort_values(ascending=False)\n",
    "bigram_indices = [word for word in sorted_words.index if ' ' in word]\n",
    "\n",
    "make_cloud(sorted_words[bigram_indices].to_dict())\n",
    "```\n",
    "\n",
    "Any extra work will add an exponential amount of power to our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
