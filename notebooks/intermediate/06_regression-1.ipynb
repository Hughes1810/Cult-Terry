{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression-1\n",
    "\n",
    "---\n",
    "\n",
    "**overview of regression for predictive modeling, and intro to classes**\n",
    "\n",
    "This will be the first notebook that moves into the machine learning / predictive analytics realm. I'll be operating on the assumption that you have some understanding of calculus and linear algrbra, but even without that the concepts should still make sense. If you skipped the prior notebook then you missed a short rant about a package called **sklearn** that will be essential for nearly everything from here on out. Tldr, if you get an error or you don't understand something that's going on then read the documentation on sklearn or copy paste into google... you aren't the first person to get that error, and someone wrote the code to throw it, which should be comforting.\n",
    "\n",
    "On a different tangent, there are two general types of machine learning: supervised and unsupervised. Supervised means \"we know what we want to predict,\" and unsupervised means \"eh.\"\n",
    "\n",
    "Regression is supervised learning as we have a known \"target\" variable that we're seeking to predict. The target that we would use a regression algorithm to predict is continuous (money, weight, height), not categorical (yes, no, dog, cat). Surprisingly, this is often a part that people get messed up with: \"which algorithm will minimize my MSE,\" is not the first thing you want to be thinking about if your data is \"person A bought a pair of doc martens,\" and your target is, \"are they going to buy a northface or a leather coat?\"\n",
    "\n",
    "**Contents:**\n",
    "1. OLS from scratch\n",
    "2. Class on Metrics\n",
    "3. In-sample vs Out-of-sample\n",
    "4. Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### OLS from scratch\n",
    "\n",
    "If you'd like to read a full explanation, [here's a good source](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf). The math hasn't changed in a long time, and I'll just be giving a brief overview before we code.\n",
    "\n",
    "OLS (linear regression, drawing a line through points) is used for solving the following question: \"I have a bunch of variables that explain my target, what's the optimal combination of these variables to predict that target?\"\n",
    "\n",
    "The optimal estimated value for each variable is expressed with the greek letter beta, with a little hat, and a subscript indicating which variable it's estimating ($\\hat{\\beta}_i$). An entire solution of betas is represented by a vector, $\\hat{\\beta}$.\n",
    "\n",
    "Linear algrbra allows for a simple solution to OLS: \n",
    "\n",
    "let $x$ be a matrix of feature variables and $y$ be a vector of target values for some observations\n",
    "\n",
    "$\\hat{\\beta} = (x^T x)^{-1} x^T y$\n",
    "\n",
    "It's quick, clean, and trivial to make into our own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy has linear algrbra (linalg) functions built in that we will take advantage of\n",
    "\n",
    "# small note, X is capitalized and y is lower case to distinguish that X is a matrix and y is a vector\n",
    "# this stands even if X only has one feature\n",
    "\n",
    "def ols(X, y):\n",
    "    '''\n",
    "    X - feature matrix\n",
    "    y - target vector\n",
    "    ---\n",
    "    returns ols betas for X and y\n",
    "    '''\n",
    "    xtx = np.dot(X.T, X)\n",
    "    inv_xtx = np.linalg.inv(xtx)\n",
    "    xty = np.dot(X.T, y)\n",
    "    \n",
    "    return np.dot(inv_xtx, xty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating some random data about tree growth rates\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "trees = np.arange(50) # 50 trees\n",
    "years = np.arange(1, 11) # over 10 years\n",
    "\n",
    "data = [(year, np.dot(year, 18) + np.random.normal(6, 6))\n",
    "        for year in years\n",
    "        for tree in trees]\n",
    "\n",
    "X = np.array([d[0] for d in data])\n",
    "y = np.array([d[1] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree height over time\n",
    "\n",
    "X_plt = X  # masking plot values is handy for later plotting\n",
    "y_plt = y\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_plt, y_plt)\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tree Height (inches)')\n",
    "plt.title('Tree Growth Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to run our regression\n",
    "\n",
    "X_model = np.c_[np.ones(X.shape[0]), X] # adding column of ones to fit y-intercept, again not messing with original X\n",
    "\n",
    "betas = ols(X_model, y)\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_plt, y_plt) # same as before\n",
    "\n",
    "b = lambda x: betas[0] + betas[1]*x # slope\n",
    "x = np.arange(min(X), max(X)+1) # range\n",
    "plt.plot(x, b(x), 'k--')\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tree Height (inches)')\n",
    "plt.title('Tree Growth Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Class on Metrics\n",
    "\n",
    "$R^2$ is meaningless because you can arbitrarily add random features to your data until you max it out. If you're ever using regression for predictive analytics and your job depends on reporting $R^2$ then - before you quit for a new job - I implore you to report adjusted $R^2$.\n",
    "\n",
    "Let's build our first python Class around a couple metrics that we think are more meaningful.\n",
    "\n",
    "* Mean Squared Error: average squared difference between targets and estimates, $\\frac{1}{n\n",
    "}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "* Mean Absolute Error: average absolute difference between targets and estimates, $\\frac{1}{n\n",
    "}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick tangent:\n",
    "\n",
    "# now that we just created regression from scratch, let's forget all of that and use sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguishing between class and function, Classes are Capitalized\n",
    "\n",
    "class Metrics:\n",
    "    \n",
    "    '''\n",
    "    metric functions using X, y, model\n",
    "    X = independent feature matrix\n",
    "    y = dependent vector / target\n",
    "    model = predictive model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, model):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.model = model\n",
    "        self.predicted = self.model.predict(self.data)\n",
    "        \n",
    "    def mse(self):\n",
    "        squared_errors = (self.target - self.predicted) ** 2\n",
    "        return np.mean(squared_errors)\n",
    "    \n",
    "    def mae(self):\n",
    "        absolute_errors = abs(self.target - self.predicted)\n",
    "        return np.mean(absolute_errors)\n",
    "        \n",
    "    \n",
    "# it's a short class, but it's incredibly useful for several reasons...\n",
    "# it can be stored in a .py script and imported into any other code\n",
    "# adding more metrics will be relatively trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for sklearn heroics\n",
    "\n",
    "X_reg = X.reshape(-1, 1)  # reshaping a 1-D array allows for modeling, pretty common trick\n",
    "y = y  # y is fine as is\n",
    "\n",
    "lr = LinearRegression() # easy-mode\n",
    "lr.fit(X_reg, y); # including the semi-colon stops sklearn from printing lr facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try out our class now\n",
    "\n",
    "metrics = Metrics(X_reg, y, lr)\n",
    "\n",
    "print('mse: {}'.format(metrics.mse()))\n",
    "print('mae: {}'.format(metrics.mae()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn can also abstract away our metrics class\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so essentially we can create our own versions of everything, but they probably exist\n",
    "# somewhere already. I would only recommend doing everything manually if you need to do\n",
    "# it for some kind of private/secret entity\n",
    "\n",
    "mse = mean_squared_error(y, lr.predict(X_reg))\n",
    "mae = mean_absolute_error(y, lr.predict(X_reg))\n",
    "\n",
    "print('mse: {}'.format(mse))\n",
    "print('mae: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### In-sample vs Out-of-sample\n",
    "\n",
    "How do we know if a model is predicting our data meaningfully? Compare it's performance on \"training data\" vs \"testing data.\"\n",
    "\n",
    "If you haven't already covered this...  When we have data and we want to build a predictive model, then we typically need to split that data randomly into 3 parts:  holdout set (used for testing a model, not training or tuning) usually 20-30% of the total data; training and validating data is the remaining 70-80%, and will be split to train and tune our model (usually via K-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some new test data for our model\n",
    "# one set of good data and one of bad\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "trees = np.arange(10) # 10 trees\n",
    "years = np.arange(1, 11) # over 10 years\n",
    "\n",
    "new_good = [(year, np.dot(year, 18) + np.random.normal(6, 6))\n",
    "            for year in years\n",
    "            for tree in trees]\n",
    "\n",
    "new_bad = [(year, np.dot(year, 18) + np.random.normal(6+6, 6)) # adding 6 simulates observations off by 4 months\n",
    "           for year in years\n",
    "           for tree in trees]\n",
    "\n",
    "X_good = np.array([d[0] for d in new_good])\n",
    "y_good = np.array([d[1] for d in new_good])\n",
    "\n",
    "X_bad = np.array([d[0] for d in new_bad])\n",
    "y_bad = np.array([d[1] for d in new_bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot (copy-pasted from above)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_plt, y_plt) # same as before\n",
    "plt.scatter(X_good, y_good, c='g')\n",
    "plt.scatter(X_bad, y_bad, c='r')\n",
    "\n",
    "b = lambda x: betas[0] + betas[1]*x # slope\n",
    "x = np.arange(min(X), max(X)+1) # range\n",
    "plt.plot(x, b(x), 'k--')\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tree Height (inches)')\n",
    "plt.title('Tree Growth Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's measure mse for both out-of-sample datas\n",
    "\n",
    "bad_mse = mean_squared_error(y_bad,\n",
    "                             lr.predict(X_bad.reshape(-1, 1)))\n",
    "good_mse = mean_squared_error(y_good,\n",
    "                              lr.predict(X_good.reshape(-1, 1)))\n",
    "\n",
    "print('bad: {}'.format(bad_mse))\n",
    "print('good: {}'.format(good_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad is clearly higher, but in the graph it doesn't look too far off\n",
    "# a direct comparison will be more useful\n",
    "\n",
    "bad_mse / mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twice as high, this definitely indicates that we should verify that the new data doesn't have any errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "With regression problems, two things pop up commonly: outlier points and high-leverage points.\n",
    "\n",
    "These phenomena will change prediction functions drastically, so it's usually better to identify and separate them before modeling the rest of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers:\n",
    "\n",
    "# let's go back to our original data (still called X and y), and change 5 points in y to be outliers\n",
    "\n",
    "y_outlier = [i+200 if idx in np.arange(240, 245)\n",
    "             else i\n",
    "             for idx, i in enumerate(y)]  # enumerate returns (index, object) for every object in a list\n",
    "\n",
    "outlier_lr = lr.fit(X_model, y_outlier)  # recall X_model is refactored for modeling\n",
    "\n",
    "outlier_betas = [outlier_lr.intercept_, outlier_lr.coef_[1]]\n",
    "outlier_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot (copy-pasted from above)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_plt, y_outlier) # same X, new y\n",
    "\n",
    "b = lambda x: betas[0] + betas[1]*x # slope regular\n",
    "b_o = lambda x: outlier_betas[0] + outlier_betas[1]*x # slope outlier\n",
    "x = np.arange(min(X), max(X)+1) # range\n",
    "\n",
    "plt.plot(x, b(x), 'k--') # without outliers\n",
    "plt.plot(x, b_o(x), 'r--') # with\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tree Height (inches)')\n",
    "plt.title('Tree Growth Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it doesn't seem too tragic a difference, but we only changed 1% of the points\n",
    "# lets compare mse with outliers to without and see what % change there is\n",
    "\n",
    "outlier_mse = mean_squared_error(y_outlier, outlier_lr.predict(X_model))\n",
    "\n",
    "print('mse ratio: {}%'.format(round(outlier_mse/mse * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Leverage Points:\n",
    "\n",
    "# essentially, what happens if the outliers are near one of the edges of our range?\n",
    "\n",
    "y_leverage = [i+200 if idx in np.arange(10, 15)\n",
    "             else i\n",
    "             for idx, i in enumerate(y)]  # enumerate returns (index, object) for every object in a list\n",
    "\n",
    "leverage_lr = lr.fit(X_model, y_leverage)  # recall X_model is refactored for modeling\n",
    "\n",
    "leverage_betas = [leverage_lr.intercept_, leverage_lr.coef_[1]]\n",
    "leverage_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot (copy-pasted from above)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_plt, y_leverage) # same X, new y\n",
    "\n",
    "b = lambda x: betas[0] + betas[1]*x # slope regular\n",
    "b_l = lambda x: leverage_betas[0] + leverage_betas[1]*x # slope outlier\n",
    "x = np.arange(min(X), max(X)+1) # range\n",
    "\n",
    "plt.plot(x, b(x), 'k--') # without outliers\n",
    "plt.plot(x, b_l(x), 'r--') # with\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tree Height (inches)')\n",
    "plt.title('Tree Growth Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this looks really bad. It's way more distinct than before, but check the mse ratio\n",
    "\n",
    "leverage_mse = mean_squared_error(y_leverage, leverage_lr.predict(X_model))\n",
    "\n",
    "print('mse ratio: {}%'.format(round(leverage_mse/mse * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in impact by the high leverage points may have been significantly more visible, but it wasn't significantly different, even about 6% less bad. So the moral of the story is if a picture is worth a thousand words, your metrics are worth a million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
