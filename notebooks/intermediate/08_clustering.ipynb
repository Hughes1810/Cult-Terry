{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "---\n",
    "\n",
    "**intro to K-Means clustering**\n",
    "\n",
    "Clustering falls into the category of \"unsupervised learning,\" i.e. we use it to explore unlabeled data (or labeled data with the labels held-out) to extract information about the shape of the data. This places an extra burden on our workload, however, since we'll need to make several assumptions about our data as well as using methods of evaluation that aren't consistently agreed upon. As a data analyst / engineer / scientist, there is always an expectation of you to communicate effectively about data and your analysis of it. Visualizing data appropriately for your audience is a key part of this, and will be a serious focus in this notebook.\n",
    "\n",
    "We'll keep this fairly elementary, but you can read more about [in the sklearn docs](https://scikit-learn.org/stable/modules/clustering.html#k-means) if you need to ramp up the difficulty.\n",
    "\n",
    "In a nutshell, the goal of K-Means is to take a given number of points (called centroids) and move them around until they are at points of best fit for the given data. \n",
    "* the sum of squared distances from each point in a cluster to its centroid is minimized: $\\sum_{i=0}^{n} \\displaystyle \\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$\n",
    "\n",
    "The solution to this formula is also called inertia, and is one of the methods used to estimate the ideal number of clusters.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "0. Exploratory Data Analysis\n",
    "1. Data Transformation\n",
    "2. Determining K\n",
    "3. Analyzing Results\n",
    "\n",
    "---\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "...often just EDA. We did this step in the last notebook, but I think it's important to mention that there is quite a range of expectation for EDA. Depending on the goal - personal use, reporting to an analytics manager, reporting to a CEO - you'll need to take a different approach to how you summarize the data that you're working with. If you're lucky, someone before you has already put in place some guidelines for this. If it's up to you, document extensively and get someone to proof-read it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take another popular dataset from uci for this exercise\n",
    "# note\n",
    "\n",
    "# source: columns copy-pasted from .name file\n",
    "\n",
    "cols = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
    "        'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "        'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                 names=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ranges of the features for this dataset are quite different, look at\n",
    "# Proline compared to any other column\n",
    "\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots aren't always the best option. seaborn is a package that has a ton\n",
    "# of visualization capabilities, one of which is the pairplot\n",
    "\n",
    "# note, this one is a bit slow and can become impossible to interpret/run at higher\n",
    "# dimensions, so be careful\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df.iloc[:,1:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Transformation\n",
    "\n",
    "We saw with df.describe that there is quite a bit of disparity bewteen the ranges of our data features. Algorithms like KNN and KMeans measure the distance between points, so without standardizing our data a small percentage change in 'Proline' will eclipse larger percentage changes in any of the other features. For certain datasets this may actually be advantageous, but since we're playing uncertain we'll need to remove this imbalance.\n",
    "\n",
    "One method is to \"normalize\" our data - force it into the standard normal distribution N(0, 1). The formula to accomplish this should look familiar to someone with a stats background: $Z = \\frac{(x - \\mu)}{\\sigma}$\n",
    "\n",
    "But we can't start off by transforming all of our data (refer to lecture about not training on testing data). First we need to train-test split, then we can normalize our training data while preserving the same $\\mu$ and $\\sigma$ to transform our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # hello again\n",
    "from sklearn import preprocessing  # contains classes for normalization/standardizing\n",
    "\n",
    "seed = 42  # always\n",
    "\n",
    "# for efficiency, I call X and y directly from the data\n",
    "# recall df.iloc[row_start:row_end, column_start:column_end]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df['Class'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = seed)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)  # fit to training data\n",
    "\n",
    "# transform both train and test with the fitted mu and sigma\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at the ranges of Proline and Alcohol (bottom left of pairplots)\n",
    "# note: a few (test) points are missing\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_train[:,0], X_train[:,-1])\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Proline');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Determining K\n",
    "\n",
    "The most popular method for determining K is \"the elbow method\" using inertia, the silhouette score, or both. \n",
    "\n",
    "We went over inertia earlier, and the silhouette score (or silhouette coefficient) isn't too far removed from that concept. To calculate it for a point, we take the mean distance between that point and all other points in its cluster ($a$) and the mean distance between that point and all other points in the nearest cluster ($b$). Then the formula $s = \\frac{b - a}{\\max(a, b)}$ gives us the coefficient for that point. Taking the mean of all silhouette coefficents gives the silhouette score.\n",
    "\n",
    "One other method is using hierarchical clustering to determine the optimal number of starting centroids and their starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "cluster_counts = [2, 3, 4, 5, 6]  # number of centroids to try\n",
    "silhouettes = []  # storage for scores\n",
    "inertias = []\n",
    "\n",
    "for count in cluster_counts:\n",
    "    \n",
    "    km = KMeans(n_clusters=count, random_state=seed)\n",
    "    cluster_labels = km.fit_predict(X_train)  # fit_predict does exactly that, fit first then predict\n",
    "    \n",
    "    silhouettes.append(silhouette_score(X_train, cluster_labels))  # s-score\n",
    "    inertias.append(km.score(X_train))  # inertia\n",
    "    \n",
    "# plot both scores per cluster on subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(cluster_counts, silhouettes)\n",
    "ax1.set_title('silhouette scores by cluster', fontweight='bold')\n",
    "ax1.set_xlabel('number of clusters')\n",
    "ax1.set_ylabel('silhouette scores')\n",
    "ax1.set_xticks([2, 3, 4, 5, 6])\n",
    "\n",
    "ax2.plot(cluster_counts, inertias)\n",
    "ax2.set_title('inertia by cluster', fontweight='bold')\n",
    "ax2.set_xlabel('number of clusters')\n",
    "ax2.set_ylabel('inertia')\n",
    "ax2.set_xticks([2, 3, 4, 5, 6])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# for silhouette score, we're looking for the global maximum score (3)\n",
    "# for inertia, we're looking for the \"elbow\" where inertia stops increasing quickly (not so clear here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more here (https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
    "X = linkage(X_train, method='ward')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(X)  # specialized plot\n",
    "plt.title('wine dendrogram', fontweight='bold')\n",
    "plt.xlabel('individual observations')\n",
    "plt.xticks([])  # don't display observation labels, it's crowded\n",
    "plt.ylabel('cluster distances')\n",
    "plt.grid(axis='y', ls='--')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# it's pretty clear that we have 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could simply stop here and initialize kmeans with 3 centroids\n",
    "# but using the centroids from our ward clustering will vastly increase our efficiency\n",
    "# - keep this method in mind if you need to cluster a large dataset\n",
    "\n",
    "centroid_members = fcluster(X, t=3, criterion='maxclust')  # list of centroids by point\n",
    "# if you want to verify, print centroid_members and \n",
    "# comment out plt.xticks([]) above to view the points\n",
    "\n",
    "centroids = {}  # simple dict creation\n",
    "\n",
    "for idx, i in enumerate(centroid_members):\n",
    "    if i not in centroids.keys():\n",
    "        centroids[i] = np.matrix(X_train[idx])  # create cluster_number: [indices]\n",
    "    else:\n",
    "        centroids[i] = np.vstack((centroids[i], X_train[idx]))  # append index to [indices]\n",
    "        \n",
    "centroid_points = [centroids[key].mean(0) for key in centroids.keys()]  # take means of each column\n",
    "\n",
    "# reference first row of single row matricies, and convert to array of lists\n",
    "centroid_points = np.asarray([i[0] for i in np.asarray(centroid_points)])\n",
    "centroid_points  # ward centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now initialize kmeans with 3 clusters at the ward centroids\n",
    "km = KMeans(n_clusters=3, random_state=seed, init=centroid_points)\n",
    "km.fit(X_train);\n",
    "\n",
    "km.cluster_centers_  # note how we've gone from 10 random inits to just 1 non-random\n",
    "\n",
    "# compare the km centers to the ward centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to copy paste this in the future,\n",
    "# it was a pain to make and should be easy to modify\n",
    "\n",
    "def pca_plot(X, y, X_main, n_components=2):\n",
    "    '''\n",
    "    get pca plot of X and y, fit on X_main, with n_components\n",
    "    \n",
    "    change colors, patches, and legend according to data\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_main)\n",
    "    pca_scores = pca.transform(X)\n",
    "\n",
    "    color_mapper = {label: idx for idx, label in enumerate(set(y))}\n",
    "    color_column = [color_mapper[label] for label in y]\n",
    "    colors = ['orange', 'blue', 'red']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))    \n",
    "    plt.scatter(pca_scores[:,0], pca_scores[:,1],\n",
    "                s=8, alpha=.8,\n",
    "                c=y,\n",
    "                cmap=matplotlib.colors.ListedColormap(colors))\n",
    "    orange_patch = matplotlib.patches.Patch(color='orange', label='cluster 1')\n",
    "    blue_patch = matplotlib.patches.Patch(color='blue', label='cluster 2')\n",
    "    red_patch = matplotlib.patches.Patch(color='red', label='cluster 3')\n",
    "    plt.legend(handles=[orange_patch, blue_patch, red_patch], prop={'size': 10})\n",
    "\n",
    "    plt.title('PCA plot of wine data')\n",
    "    plt.xlabel('PCA dimension 1')\n",
    "    plt.ylabel('PCA dimension 2')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "pca_plot(X_train, y_train, X_train)  # n_components=2 so we can get a flat plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the clusters definitely aren't perfect, and if we wanted to increase performance\n",
    "# we would go back to our data transformation step and do some dimensionality reduction\n",
    "# to reduce the \"curse of dimensionality\"\n",
    "\n",
    "\n",
    "# let's see how we do on the testing data though\n",
    "pca_plot(X_test, y_test, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
