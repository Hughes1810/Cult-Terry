{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-1\n",
    "\n",
    "---\n",
    "\n",
    "**intro to classification with nearest neighbors**\n",
    "\n",
    "In contrast to regression, classification is used when we need to predict if an observation falls into a discrete bin (yes or no, tree or flower). In this notebook we'll go over a fundamental classification algorithm - K Nearest Neighbors - by creating it from scratch before implementing it with sklearn, tuning parameters, and generating some visualizations.\n",
    "\n",
    "In practice, i.e. outside of these notebooks, the most important thing you can do as a problem-solving-person is know your data intimately. Without understanding what you're working with on a fundamental level, you'll only be able to derive valuable insights if you get lucky. Because I love metaphor: building a model without understanding your data is like building a bridge out of silver and gold. It might look pretty, but those solid gold I-beams will buckle as soon as you sneeze in their direction. \n",
    "\n",
    "So, before we get into the guts of this one, we'll take a \"step 0.\"\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "0. Data Exploration\n",
    "1. KNN from scratch\n",
    "2. sklearn, metrics, visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in some data, iris is a classic dataset\n",
    "# you can copy the link (remove /iris.data) to view the source\n",
    "\n",
    "cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                 names = cols)\n",
    "\n",
    "df.head()  # you probably noticed I use .head(), you may prefer .sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"class\" to make binary variables so we can visualize the distribution\n",
    "\n",
    "# dummy variables are a way to code categorical variables for computation,\n",
    "# if we were using \"class\" to build a model we would hold-out one of the\n",
    "# levels to avoid perfect collinearity\n",
    "\n",
    "class_dummies = pd.get_dummies(df['class']) \n",
    "\n",
    "class_dummies.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now concatenate on columns and describe()\n",
    "# without the dummies, we wouldn't be able to show any metrics for \"class\"\n",
    "\n",
    "df = pd.concat([df, class_dummies], axis=1)\n",
    "\n",
    "df.describe().round(3)  # df.round(n) is helpful for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our classes are all evenly distributed, we can change 'class' into a numeric variable for later\n",
    "\n",
    "df['class'] = [0 if c == 'Iris-setosa' \n",
    "               else 1 if c == 'Iris-versicolor'  # it's possible to do elif in a list comprehension\n",
    "               else 2  # no if for the last else\n",
    "               for c in df['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not the best at mental math or holding all of these numbers in my head\n",
    "# so I prefer to have a visualization to point to, rather than recalling which\n",
    "# variables had outliers and what the boundaries are\n",
    "\n",
    "# let's make box plots of the four measurement variables\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axs[0, 0].boxplot(df['sepal_length'])\n",
    "axs[0, 0].set_title('sepal length')\n",
    "\n",
    "axs[0, 1].boxplot(df['sepal_width'])\n",
    "axs[0, 1].set_title('sepal width')\n",
    "\n",
    "axs[1, 0].boxplot(df['petal_length'])\n",
    "axs[1, 0].set_title('petal length')\n",
    "\n",
    "axs[1, 1].boxplot(df['petal_width'])\n",
    "axs[1, 1].set_title('petal width')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few points in sepal width that might be outliers, but nothing egregious.\n",
    "# it's the most spread-out distribution (smallest box:graph ratio) so it may just\n",
    "# have a higher threshold (3 std instead of 1.5 or 2)\n",
    "\n",
    "# we can make two logical plots, sepal size (l*w) and petal size (l*w)\n",
    "# that will give us a clearer visual if there are any points that really stand out\n",
    "\n",
    "colors = ['g' if c == 0  # assigning a color vector based on \"class\"\n",
    "          else 'c' if c == 1  \n",
    "          else 'm'  \n",
    "          for c in df['class']]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))  # two horizontal subplots\n",
    "\n",
    "# sepal width v height\n",
    "ax1.scatter(df['sepal_width'], df['sepal_length'], color=colors)\n",
    "ax1.set_title('sepal size', fontsize=16)\n",
    "ax1.set_xlabel('sepal width', fontsize=16)\n",
    "ax1.set_ylabel('sepal height', fontsize=16)\n",
    "ax1.grid()\n",
    "\n",
    "# petal width v height\n",
    "ax2.scatter(df['petal_width'], df['petal_length'], color=colors)\n",
    "ax2.set_title('petal size', fontsize=16)\n",
    "ax2.set_xlabel('petal width', fontsize=16)\n",
    "ax2.set_ylabel('petal height', fontsize=16)\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviating from the practical approach here - solving this problem with petal data would be trivial, and if this was a \"real-world\" problem then we would be extremely excited to see such a clearly good option - since the sepal data is messier and more realistic, we'll use that later on. But first...\n",
    "\n",
    "---\n",
    "\n",
    "### KNN from scratch\n",
    "\n",
    "Nearest neighbors is a logically intuitive algorithm even if you don't go through it step by step. It's simple pattern recognition that mimics the idea that things near each other are like each other. For example: plants in a desert are like to other plants found in a desert, and not like plants in a rainforest.\n",
    "\n",
    "The K in K Nearest Neighbors can be any natural number that you want. The ideal K for a dataset will depend on the number of observations and how they're spread out across the dimensions of interest. Generally speaking, a lower K leads to a more sensitive and \"wild looking\" decision boundary while a higher K leads to a smoother, less tolerant boundary (unless you go too high, where one class swallows the rest).\n",
    "\n",
    "Digging into K, we may want to look at how we use those nearest neighbors to make a prediction about our observed point. A simple majority is the most basic version, and the one we're about to create, but stepping from there into other metrics (like taking a mean) is actually quite trivial.\n",
    "\n",
    "**The logical steps for KNN:**\n",
    "1. choose a K\n",
    "2. for point $i_n$, calculate the distance to all other observations\n",
    "3. sort, and keep the K closest observations\n",
    "4. use the target values of the observations to predict $y_{i_n}$\n",
    "5. repeat 2-5 for all $i_n$\n",
    "\n",
    "Note: in step 2, KNN may start to break-down or lose useful interpretability at higher dimensions due to \"the curse of dimensionality.\" To imagine the curse, think of two points on a line about 1 inch apart, then make the line a plane where they're also 1 inch apart on the second axis. Now they're $\\sqrt{1^2 + 1^2}$ inches apart, at 4 dimensions they're $\\sqrt{1^2 + 1^2 + 1^2 + 1^2} = 2$ inches apart, and so on until so many dimensions are added that even a large number of random points will all be very far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: assume K = 2, that was easy\n",
    "\n",
    "# step 2: calculate all distances between a point and every other point\n",
    "\n",
    "# say we have the following 7 points. We know 3 are in blue group, and 3 are in red,\n",
    "# green we're unsure about:\n",
    "\n",
    "ex_points = [np.array((1, 1)),\n",
    "             np.array((2, 2)),\n",
    "             np.array((3, 2)),\n",
    "             np.array((2, 5)),\n",
    "             np.array((4, 4)),\n",
    "             np.array((4, 5)),\n",
    "             np.array((5, 4))]\n",
    "\n",
    "colors = ['b']*3 + ['g'] + ['r']*3\n",
    "\n",
    "# now plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter([p[0] for p in ex_points],\n",
    "            [p[1] for p in ex_points],\n",
    "            c=colors,\n",
    "            s=48)\n",
    "\n",
    "plt.xlabel('x', fontsize=16)\n",
    "plt.ylabel('y', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can loop through all of the points, finding distances between all others\n",
    "\n",
    "distance_matrix = []\n",
    "\n",
    "for point in ex_points:\n",
    "    point_dists = []  # these will be our rows\n",
    "    for x in ex_points:  \n",
    "        point_dists.append(np.linalg.norm(point - x))  # the norm of the difference of two vectors is their distance\n",
    "    distance_matrix.append(point_dists)\n",
    "    \n",
    "dist_df = pd.DataFrame(data=distance_matrix)  # matrix to df\n",
    "print(dist_df.shape)  # print a callable tuple of the lengths of a dataframe's (rows, columns)\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can go through the distances and find the shortest ones\n",
    "# note: each point is closest to itself, but a point can't be its own neighbor\n",
    "\n",
    "nearest_neighbor = []\n",
    "\n",
    "for idx in range(dist_df.shape[0]):  # iterate through row indices\n",
    "    row = dist_df.iloc[idx]  # locate index\n",
    "    row = row.drop(idx)  # drop self\n",
    "    nearest_neighbor.append(row.idxmin())  # get the index of the minimum distance\n",
    "    \n",
    "nearest_neighbor  # verify against our above df or plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to expand to nearest two neighbors\n",
    "\n",
    "nearest_two = []\n",
    "\n",
    "for idx in range(dist_df.shape[0]):\n",
    "    row = dist_df.iloc[idx].drop(idx)  # combine two steps to clean up\n",
    "    row = row.sort_values()  # sort to get more than just the min index\n",
    "    near_idx = row.index[:2]  # row.index retrieves the indices as a series, we want 2\n",
    "    nearest_two.append([i for i in near_idx])  # we can pull out the indices with a list comprehension\n",
    "    \n",
    "nearest_two  # again it's fairly trivial to scroll up and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually, we can check what class our green point is in\n",
    "# but let's create our own little classifier\n",
    "\n",
    "classes = []\n",
    "\n",
    "for n in nearest_two:  # each list within the list\n",
    "    c = []\n",
    "    for idx in n:  # each number is an index\n",
    "        c.append(colors[idx])  # here colors are 'classes'\n",
    "    if len(set(c)) > 1:  # mix of 'r' and 'b'\n",
    "        classes.append('g')\n",
    "    else:  # only 'r' or 'b'\n",
    "        classes.append(c[0])  \n",
    "        \n",
    "# copy paste and plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter([p[0] for p in ex_points],\n",
    "            [p[1] for p in ex_points],\n",
    "            c=classes,  # only change\n",
    "            s=48)\n",
    "\n",
    "plt.xlabel('x', fontsize=16)\n",
    "plt.ylabel('y', fontsize=16)\n",
    "\n",
    "plt.show()  # voila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Therefore**...\n",
    "\n",
    "we should classify our 'uncertain' point as red, since the closest two points are 5 and 4 (both red).\n",
    "\n",
    "---\n",
    "\n",
    "### sklearn, metrics, visualizations\n",
    "\n",
    "Now that we have a solid understanding of what's going on under the hood, it's time to pull out sklearn and bring our flower data back into the mix.\n",
    "\n",
    "In keeping with a practical lens, we'll need to train-test split our data before we train a model.\n",
    "\n",
    "**Important: Do not use testing data to fit/train a model.** \n",
    "\n",
    "In order to determine which K is going to be the best for predicting species, we need some kind of metric to make a decision against. To keep things simple, we'll use a confusion matrix and our best judgement. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. train-test split (80-20 is typical)\n",
    "2. with train, generate models for some K's\n",
    "3. use metrics & visualizations to determine best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier is what we need right now, we can also use KNN to solve regression problems\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # import for splitting\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix  # current best option for plotting a confusion matrix\n",
    "\n",
    "seed = 42  # for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that the sepal data is messier, so we'll use that for this exercise\n",
    "\n",
    "X = df[['sepal_length', 'sepal_width']]  # matrix of features\n",
    "\n",
    "y = df['class']  # vector of targets\n",
    "\n",
    "\n",
    "# you can hit shift+tab on \"train_test_split\" for more info, but the high-level\n",
    "# explanation is that we're randomly & reproducibly splitting up our data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = seed)\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize KNN with differen K's\n",
    "\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_15 = KNeighborsClassifier(n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run through knn_2\n",
    "\n",
    "# start by fitting\n",
    "\n",
    "knn_2.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a confusion matrix to see how well the model fits\n",
    "# create a function so we can run this for the other knn versions as well\n",
    "\n",
    "def plot_cm(y_true, y_pred):\n",
    "    '''\n",
    "    using a fitted model, plot a confusion matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm)                       \n",
    "    plt.show()\n",
    "\n",
    "# plot\n",
    "plot_cm(y_train, knn_2.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can already get a good sense for how well the model is working,\n",
    "# but a visualization of the decision boundary will really help us out\n",
    "\n",
    "from matplotlib.colors import ListedColormap  # allows us to use our own color selection\n",
    "\n",
    "# plotting function - nice to do this when the plot is reusable and the code is messy\n",
    "\n",
    "def plot_knn(X, y, model, title):\n",
    "    \n",
    "    # use color-blind friendly colors - http://mkweb.bcgsc.ca/colorblind/\n",
    "    # hex color source - https://htmlcolorcodes.com/color-picker/\n",
    "    point_color = ListedColormap(['#FF0000', '#0000FF', '#FFFF00'])\n",
    "    back_color = ListedColormap(['#FF9696', '#9696FF', '#FFFF96'])\n",
    "    \n",
    "    # create a background mesh of points to show the decision boundary \n",
    "    # - if you copypaste this, make sure to change cushion and step to suit your data\n",
    "    cushion = 0.5\n",
    "    x_min, x_max = X.iloc[:, 0].min() - cushion, X.iloc[:, 0].max() + cushion\n",
    "    y_min, y_max = X.iloc[:, 1].min() - cushion, X.iloc[:, 1].max() + cushion\n",
    "    \n",
    "    step = 0.02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n",
    "                         np.arange(y_min, y_max, step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # now plot the background mesh\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=back_color)\n",
    "\n",
    "    # and scatterplot the points\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1],\n",
    "                c=y, cmap=point_color,\n",
    "                edgecolor='k', linewidths=2, s=36)\n",
    "    plt.title(title, fontsize=24)\n",
    "\n",
    "    plt.show()\n",
    "      \n",
    "\n",
    "plot_knn(X_train, y_train, knn_2, 'K = 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can loop through the other 4 K's, plotting as we go\n",
    "# notice how the decision boundary changes as K goes up\n",
    "\n",
    "for model in [knn_3, knn_5, knn_10, knn_15]:\n",
    "    \n",
    "    model.fit(X_train, y_train);\n",
    "    plot_cm(y_train, model.predict(X_train))\n",
    "    plot_knn(X_train, y_train, model, title='K = {}'.format(model.get_params()['n_neighbors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to use a model to predict our testing data\n",
    "\n",
    "# choose \"optimal_model\" based on the matricies and visuals above\n",
    "\n",
    "optimal_model = knn_15\n",
    "\n",
    "plot_cm(y_test, optimal_model.predict(X_test))\n",
    "plot_knn(X_test, y_test, optimal_model, title='K = {}'.format(optimal_model.get_params()['n_neighbors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
