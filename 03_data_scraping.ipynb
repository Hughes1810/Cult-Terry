{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Quickstart Tutorial\n",
    "---\n",
    "Install:  \n",
    "* [Selenium](https://www.seleniumhq.org/download/)\n",
    "* [ChromeDriver](https://chromedriver.chromium.org/downloads)\n",
    "---\n",
    "\n",
    "**Selenium, accessing data that can't just be downloaded**\n",
    "\n",
    "Summary:  \n",
    "1. basic Google search\n",
    "2. scraping exercise - jobs\n",
    "\n",
    "**Special disclaimer: you should always check the robots.txt file for a website before you scrape it. [This article](https://moz.com/learn/seo/robotstxt) does a better job explaing than I'll be able to.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# storage\n",
    "import pickle  # pickle is one option for storage when you aren't using \"df.to_(csv/excel/...)\"\"\n",
    "\n",
    "# selenium-related imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# prep-work\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a Chrome window with google, this will open a new window \"being controlled by test software\"\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver) # webdriver executable\n",
    "driver.get('https://www.google.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To open up the inspector: cmnd + shift + C**\n",
    "\n",
    "Now hover over the search bar and click on it.  \n",
    "In the inspector there is a large chunk of text that starts with:  \n",
    "```<input class=```  \n",
    "Right click, go to \"Copy XPath\" ...\n",
    "\n",
    "```//*[@id=\"tsf\"]/div[2]/div[1]/div[1]/div/div[2]/input```\n",
    "\n",
    "A bit messy, but XPath is generally a safe bet when you need Selenium to click on the right thing.\n",
    "\n",
    "However, looking back at the text chunk, there's a field ```name=\"q\"```. This is one example where XPath isn't the best bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_name('q')\n",
    "\n",
    "# sleeps help to make your bot appear more 'human'\n",
    "time.sleep(1);\n",
    "\n",
    "# now send in a search...\n",
    "search_bar.send_keys('data science jobs')\n",
    "# and hit enter (after 'Keys.' you can hit TAB to check out your options - this is true after any '.')\n",
    "search_bar.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:**  \n",
    "If you don't explicitly call ```driver.quit()``` or ```right-click + quit``` the browser tab, then your driver tabs will remain open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Scraping Exercise - Monster\n",
    "\n",
    "Job listings are extremely dynamic, and Monster has pretty lax rules for scraping data from their site. Our objective is to collect data on listed jobs (job descriptions) and analyze it.\n",
    "\n",
    "Manually trawling posts would be tedious, so...\n",
    "\n",
    "---\n",
    "#### 2.1 One Job\n",
    "\n",
    "As with all problems that involve repetition, do it once (or a handful of times) at first. Figure out the optimal-ish way to do the task and then loop through it as many times as you need.\n",
    "\n",
    "Start off by [going to monster](https://www.monster.com/).\n",
    "\n",
    "For the rest of this exercise, I'll walk you through a scrape of data science job listings. Feel free to change the parameters to your own specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we go into developer mode, before we open a driver, let's look at search filters\n",
    "# I've chosen full time jobs listed in the last week\n",
    "\n",
    "link = 'https://www.monster.com/jobs/search/Full-Time_8?q=data-scientist&tm=7'\n",
    "\n",
    "# my search returns ~2k listings, more than enough to be \"statistically relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmnd + shift + C to check the xpath for the first posting\n",
    "\n",
    "job_start = '//*[@id=\"SearchResults\"]/section[1]' # starts at 1\n",
    "\n",
    "# cmnd + shift + C to check the last\n",
    "\n",
    "job_end = '//*[@id=\"SearchResults\"]/section[29]'\n",
    "\n",
    "# only 29 postings show before \"Load more jobs\"\n",
    "\n",
    "more_jobs = '//*[@id=\"loadMoreJobs\"]'\n",
    "\n",
    "# scroll to the last one\n",
    "\n",
    "new_job_end = '//*[@id=\"SearchResults\"]/section[57]' # 57?\n",
    "\n",
    "# we started with 29 jobs, and we get 28 every time we ask for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have everything that we need, let's do a test run\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "driver.get(link) # link with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_job = driver.find_element_by_xpath(job_start) # find the path we coppied for job 1\n",
    "\n",
    "first_job.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmnd + shift + C\n",
    "\n",
    "header_xpath = '//*[@id=\"JobPreview\"]/div[1]/div[1]'\n",
    "\n",
    "description_xpath = '//*[@id=\"JobDescription\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .text is eponymous (no parantheses, the text isn't a function) \n",
    "\n",
    "driver.find_element_by_xpath(header_xpath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice a tweet from the description, looks good\n",
    "\n",
    "driver.find_element_by_xpath(description_xpath).text[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.2 All the jobs\n",
    "\n",
    "All of the components are there for us. Now it's time to put it all together.\n",
    "\n",
    "Our loop should look like this:\n",
    "1. scrape jobs 1-29\n",
    "2. click on \"load more jobs\"\n",
    "3. scrape the next 28 jobs\n",
    "4. repeat steps 2-3 until we have our desired number of job listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of our paths\n",
    "\n",
    "link = 'https://www.monster.com/jobs/search/Full-Time_8?q=data-scientist&tm=7'\n",
    "\n",
    "job_n = '//*[@id=\"SearchResults\"]/section[{}]' # note the \"{}\"\n",
    "\n",
    "header_xpath = '//*[@id=\"JobPreview\"]/div[1]/div[1]'\n",
    "\n",
    "description_xpath = '//*[@id=\"JobDescription\"]'\n",
    "\n",
    "more_jobs = '//*[@id=\"loadMoreJobs\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to come up with a way to loop through the posting numbers\n",
    "\n",
    "# 29, 57, ...\n",
    "# it looks like the numbers pre-button are a multiple of 28 + 1\n",
    "\n",
    "29 % 28, 57 % 28  # remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do have a corner case\n",
    "\n",
    "1 % 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double filter, and verify that the printed numbers match our target click-indices\n",
    "\n",
    "for i in np.arange(1, 100):\n",
    "    if i % 28 == 1 and i > 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a ceiling at 100 jobs\n",
    "# note: this cell will error\n",
    "\n",
    "job_headers = []\n",
    "job_descriptions = []\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(link)\n",
    "time.sleep(5) # letting the page load\n",
    "\n",
    "for n in np.arange(1, 101): # remember that range cuts at end-1\n",
    "    # click job_n\n",
    "    job_path = driver.find_element_by_xpath(job_n.format(n))\n",
    "    job_path.click()\n",
    "    time.sleep(2)  # resting after clicks\n",
    "    # get_data\n",
    "    job_headers.append(driver.find_element_by_xpath(header_xpath).text)\n",
    "    job_descriptions.append(driver.find_element_by_xpath(description_xpath).text)\n",
    "    # \"more jobs\"\n",
    "    if i % 28 == 1 and i > 1:\n",
    "        driver.find_element_by_xpath(more_jobs).click()\n",
    "        time.sleep(2)  # sometimes it's benefical to make these random\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job path wasn't real... hmm\n",
    "# check the xpath for job 2\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(link)\n",
    "\n",
    "job_2 = '//*[@id=\"SearchResults\"]/section[3]' # lmao, so that's why the numbering was weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing as last time\n",
    "\n",
    "job_headers = []\n",
    "job_descriptions = []\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(link)\n",
    "time.sleep(5) # letting the page load\n",
    "\n",
    "# we have a few options, the easiest is a try, except statement\n",
    "# try to do what we wanted, except move on\n",
    "# there may be other numbers missing, and we don't want to write an exception for each\n",
    "# that said, let's switch to a while loop and count to 100\n",
    "\n",
    "i = 0\n",
    "while i < 100:\n",
    "    n = 1  # new job number iterator\n",
    "    try: \n",
    "        # click job_n\n",
    "        job_path = driver.find_element_by_xpath(job_n.format(n))\n",
    "        job_path.click()\n",
    "        time.sleep(2)  # resting after clicks\n",
    "        # get_data\n",
    "        job_headers.append(driver.find_element_by_xpath(header_xpath).text)\n",
    "        job_descriptions.append(driver.find_element_by_xpath(description_xpath).text)\n",
    "        # \"more jobs\"\n",
    "        if n % 28 == 1 and n > 1:\n",
    "            driver.find_element_by_xpath(more_jobs).click()\n",
    "            time.sleep(3)  # sometimes it's benefical to make these random\n",
    "        i += 1  # increment for each success\n",
    "    except:\n",
    "        pass  # we could also print, or try some other function\n",
    "    n += 1  # since we're counting jobs not successes\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that we got 100\n",
    "\n",
    "len(job_descriptions), len(job_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3 Data Storage\n",
    "\n",
    "Pickling is just one way to handle data storage, but it's effective for random data like we just grabbed.\n",
    "\n",
    "If you have some data that's already in a dataframe, then using pandas to_csv is far superior. Csv files also have the advantage of being easily transportable across environments to pretty much any end user (there's also a to_excel call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets set everything into a dictionary\n",
    "\n",
    "job_dict = {j: [job_headers[j], job_descriptions[j]] for j in range(100)}\n",
    "\n",
    "# now \"dump\" (object to dump, open(filename for dumping, write as a binary file))\n",
    "\n",
    "pickle.dump(job_dict, open('jobs.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
